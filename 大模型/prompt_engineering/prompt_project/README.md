

[提示词工程](https://www.promptingguide.ai/zh)

# 提示工程指南
提示工程(Prompt Engineering) 关注提示词的开发和优化，帮助用户将大语言模型(Large Language Model, LLM) 用于各场景和研究领域。 掌握了提示工程相关技能将有助于用户更好地了解大模型语言的能力和局限性。 

提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。用户可以通过提示工程来提高大语言模型的安全性，也可以赋能大语言模型，比如借助专业领域知识和外部工具来增强大语言模型能力。

基于对大语言模型的浓厚兴趣，我们编写了这份全新的提示工程指南，介绍了大语言模型相关的论文研究、学习指南、模型、讲座、参考资料、大语言模型能力以及与其他与提示工程相关的工具。

# 提示工程简介
## 大模型设置
Temperature：简单来说，temperature 的参数值越小，模型就会返回越确定的一个结果。如果调高该参数值，大语言模型可能会返回更随机的结果，也就是说这可能会带来更多样化或更具创造性的产出。（调小temperature）实质上，你是在增加其他可能的 token 的权重。在实际应用方面，对于质量保障（QA）等任务，我们可以设置更低的 temperature 值，以促使模型基于事实返回更真实和简洁的结果。 对于诗歌生成或其他创造性任务，适度地调高 temperature 参数值可能会更好。

Top_p：同样，使用 top_p（与 temperature 一起称为核采样（nucleus sampling）的技术），可以用来控制模型返回结果的确定性。如果你需要准确和事实的答案，就把参数值调低。如果你在寻找更多样化的响应，可以将其值调高点。

使用Top P意味着只有词元集合（tokens）中包含top_p概率质量的才会被考虑用于响应，因此较低的top_p值会选择最有信心的响应。这意味着较高的top_p值将使模型考虑更多可能的词语，包括不太可能的词语，从而导致更多样化的输出。

一般建议是改变 Temperature 和 Top P 其中一个参数就行，不用两个都调整。

Max Length：您可以通过调整 max length 来控制大模型生成的 token 数。指定 Max Length 有助于防止大模型生成冗长或不相关的响应并控制成本。

Stop Sequences：stop sequence 是一个字符串，可以阻止模型生成 token，指定 stop sequences 是控制大模型响应长度和结构的另一种方法。例如，您可以通过添加 “11” 作为 stop sequence 来告诉模型生成不超过 10 个项的列表。

Frequency Penalty：frequency penalty 是对下一个生成的 token 进行惩罚，这个惩罚和 token 在响应和提示中已出现的次数成比例， frequency penalty 越高，某个词再次出现的可能性就越小，这个设置通过给 重复数量多的 Token 设置更高的惩罚来减少响应中单词的重复。

Presence Penalty：presence penalty 也是对重复的 token 施加惩罚，但与 frequency penalty 不同的是，惩罚对于所有重复 token 都是相同的。出现两次的 token 和出现 10 次的 token 会受到相同的惩罚。 此设置可防止模型在响应中过于频繁地生成重复的词。 如果您希望模型生成多样化或创造性的文本，您可以设置更高的 presence penalty，如果您希望模型生成更专注的内容，您可以设置更低的 presence penalty。

与 temperature 和 top_p 一样，一般建议是改变 frequency penalty 和 presence penalty 其中一个参数就行，不要同时调整两个。

在我们开始一些基础示例之前，请记住最终生成的结果可能会和使用的大语言模型的版本而异。

## 基本概念

## 提示词要素
提示词可以包含以下任意要素：
指令：想要模型执行的特定任务或指令
上下文：包含外部信息或额外的上下文信息，引导语言模型更好地响应
输入数据：用户输入的内容或问题
输出指示：输出的类型或格式

## 提示词设计的通用技巧
### 从简单开始
在开始设计提示时，你应该记住，这实际上是一个迭代过程，需要大量的实验才能获得最佳结果。使用来自OpenAI或Cohere的简单的 playground 是一个很好的起点。

你可以从简单的提示词开始，并逐渐添加更多元素和上下文（因为你想要更好的结果）。因此，在这个过程中不断迭代你的提示词是至关重要的。阅读本指南时，你会看到许多示例，其中具体性、简洁性和简明性通常会带来更好的结果。

当你有一个涉及许多不同子任务的大任务时，可以尝试将任务分解为更简单的子任务，并随着结果的改善逐步构建。这避免了在提示设计过程中一开始就添加过多的复杂性。
### 指令
你可以使用命令来指示模型执行各种简单任务，例如“写入”、“分类”、“总结”、“翻译”、“排序”等，从而为各种简单任务设计有效的提示。

请记住，你还需要进行大量实验以找出最有效的方法。以不同的关键词（keywords），上下文（contexts）和数据（data）试验不同的指令（instruction），看看什么样是最适合你特定用例和任务的。通常，上下文越具体和跟任务越相关则效果越好。在接下来的指南中，我们将讨论样例和添加更多上下文的重要性。

有些人建议将指令放在提示的开头。另有人则建议是使用像“###”这样的清晰分隔符来分隔指令和上下文。

例如：

提示：
```
### 指令 ###
将以下文本翻译成西班牙语：
文本：“hello！”
```
输出：
```
¡Hola!
```
### 具体性
要非常具体地说明你希望模型执行的指令和任务。提示越具描述性和详细，结果越好。特别是当你对生成的结果或风格有要求时，这一点尤为重要。不存在什么特定的词元（tokens）或关键词（tokens）能确定带来更好的结果。更重要的是要有一个具有良好格式和描述性的提示词。事实上，在提示中提供示例对于获得特定格式的期望输出非常有效。

在设计提示时，还应注意提示的长度，因为提示的长度是有限制的。想一想你需要多么的具体和详细。包含太多不必要的细节不一定是好的方法。这些细节应该是相关的，并有助于完成手头的任务。这是你需要进行大量实验的事情。我们鼓励大量实验和迭代，以优化适用于你应用的提示。

例如，让我们尝试从一段文本中提取特定信息的简单提示。

提示：
```
提取以下文本中的地名。
所需格式：
地点：<逗号分隔的公司名称列表>
输入：“虽然这些发展对研究人员来说是令人鼓舞的，但仍有许多谜团。里斯本未知的香帕利莫德中心的神经免疫学家 Henrique Veiga-Fernandes 说：“我们经常在大脑和我们在周围看到的效果之间有一个黑匣子。”“如果我们想在治疗背景下使用它，我们实际上需要了解机制。””
```
输出：
```
地点：里斯本，香帕利莫德中心
```
### 明确
给定上述关于详细描述和改进格式的建议，很容易陷入陷阱：想要在提示上过于聪明，从而可能创造出不明确的描述。通常来说，具体和直接会更好。这里的类比非常类似于有效沟通——越直接，信息传达得越有效。

例如，你可能有兴趣了解提示工程的概念。你可以尝试这样做：
```
解释提示工程的概念。保持解释简短，只有几句话，不要过于描述。
```
从上面的提示中不清楚要使用多少句子以及什么风格。尽管你可能仍会从上述提示中得到较好的响应，但更好的提示应当是非常具体、简洁并且切中要点的。例如：
```
使用 2-3 句话向高中学生解释提示工程的概念。
```
### 做什么还是不做什么
设计提示时的另一个常见技巧是避免说不要做什么，而应该说要做什么。这样（说要做什么）更加的具体，并且聚焦于（有利于模型生成良好回复的）细节上。

以下是一个电影推荐聊天机器人的示例，因为我写的指令——关注于不要做什么，而失败了。

提示：
```
以下是向客户推荐电影的代理程序。不要询问兴趣。不要询问个人信息。
客户：请根据我的兴趣推荐电影。
代理：
```
输出：
```
当然，我可以根据你的兴趣推荐电影。你想看什么类型的电影？你喜欢动作片、喜剧片、爱情片还是其他类型的电影？
```
以下是更好的提示：

提示：
```
以下是向客户推荐电影的代理程序。代理负责从全球热门电影中推荐电影。它应该避免询问用户的偏好并避免询问个人信息。如果代理没有电影推荐，它应该回答“抱歉，今天找不到电影推荐。”。
```顾客：请根据我的兴趣推荐一部电影。
客服：
```
输出：
```
抱歉，我没有关于你兴趣的任何信息。不过，这是目前全球热门的电影列表：[电影列表]。希望你能找到喜欢的电影！
```
上述示例中的一些内容摘自 "Best practices for prompt engineering with OpenAI API" article.

# 提示技术
## 零样本提示(Zero Shot)
零样本能力 - 在提示词中，不向模型提供任何示例


指令调整可以改善零样本学习
指令调整本质上是通过指令描述的数据集上微调模型

提示：
```
将文本分类为中性、负面或正面。
文本：我认为这次假期还可以。
情感：
```
输出
```
中性
```

零样本提示基于指令调整，也即是模型微调技术

如果零样本技术不起作用，可以在提示中提供演示或者示例，这就引出了少样本提示

## 少样本提示(Few Shot)
少样本提示：作为一种技术，启用上下文学习， 在提示中提供演示以引导模型实现更好的性能。 



提示
```
“whatpu”是坦桑尼亚的一种小型毛茸茸的动物。一个使用whatpu这个词的句子的例子是：
我们在非洲旅行时看到了这些非常可爱的whatpus。
“farduddle”是指快速跳上跳下。一个使用farduddle这个词的句子的例子是：
```
输出
```
当我们赢得比赛时，我们都开始庆祝跳跃。
```

上述例子中，模型通过要给示例(1-shot) 学会了如何执行任务。对于更加困难的任务，可以尝试增演示(例如：3-shot, 5-shot, 10-shot等)

### 少样本提示的限制
标准的少样本提示对许多任务都有效，但仍然不是一种完美的技术，特别是在处理更加复杂的推理任务时。

## 思维链(Chain-of-Thought Prompting)
